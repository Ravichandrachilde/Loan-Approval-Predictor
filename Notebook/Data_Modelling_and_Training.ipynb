{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlXk47ahCgB2"
      },
      "source": [
        "**NOTE:** **I had to clear the outputs because github wasn't showing the code for the notebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qHnIB6CmF4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6COYIJGsriYD"
      },
      "outputs": [],
      "source": [
        "# 1. Load Data\n",
        "data = pd.read_csv('/content/drive/MyDrive/loan predictor/loan_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXzgC5wjrs84"
      },
      "outputs": [],
      "source": [
        "#Check Column Names\n",
        "print(f\"Columns: {data.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvhm2F9OXlLT"
      },
      "source": [
        "Found spaces at beginning of column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EohyBGw0sFbk"
      },
      "outputs": [],
      "source": [
        "# Remove spaces in coulmn names\n",
        "data.columns = data.columns.str.strip()\n",
        "print(f\"Columns: {data.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrozDZh-t_vO"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['loan_id'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33q_30zdscC9"
      },
      "outputs": [],
      "source": [
        "# found spaces in output of loan_status column and Mapping the output of target variable\n",
        "data['loan_status'] = data['loan_status'].str.strip()\n",
        "data['loan_status'] = data['loan_status'].map({'Approved': 1, 'Rejected': 0})\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW8GIZbJxJWT"
      },
      "outputs": [],
      "source": [
        "numeric_cols = data.select_dtypes(include=['number']).columns\n",
        "print(\"Minimum values per column:\")\n",
        "print(data[numeric_cols].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGXle6pBPSu"
      },
      "source": [
        "Found -ve values in res. assets column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9TKVeXVBYxh"
      },
      "outputs": [],
      "source": [
        "neg_mask = data['residential_assets_value'] < 0\n",
        "bad_rows = data[neg_mask]\n",
        "\n",
        "print(f\"Number of negative rows: {len(bad_rows)}\")\n",
        "print(\"Unique negative values:\")\n",
        "print(bad_rows['residential_assets_value'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4-LT1BVCYa8"
      },
      "outputs": [],
      "source": [
        "display(bad_rows.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3znmby5gH3VS"
      },
      "source": [
        "Looks more like a system error to me than anyother problem like debt.\n",
        "\n",
        "I'm saying this because when you look at some data, even though the loan_status was approved or rejected. they all have same number and if we think it's debt not all of them should be having same debt amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3vJzInSGuVL"
      },
      "outputs": [],
      "source": [
        "data = data[data['residential_assets_value'] != -100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0XKUhxOG4W3"
      },
      "outputs": [],
      "source": [
        "data['residential_assets_value'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLfiZyqutuLO"
      },
      "outputs": [],
      "source": [
        "print(f\"Duplicates: {data.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBV3pgpjzPoY"
      },
      "outputs": [],
      "source": [
        "X = data.drop('loan_status', axis=1)\n",
        "y = data['loan_status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF5WPIKIdbBJ"
      },
      "source": [
        "Splitting data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHicVmzi3sO4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratify=y because of the class imbalance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYZzHCCizdJF"
      },
      "outputs": [],
      "source": [
        "print(\"Target Class Distribution :\")\n",
        "y_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5VpZledh89"
      },
      "source": [
        "Found Class Imbalance between approved and rejected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OGnur4V1PoC"
      },
      "outputs": [],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgU3duMAsZCO"
      },
      "outputs": [],
      "source": [
        "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySkl3Y9ZPOzG"
      },
      "outputs": [],
      "source": [
        "print(f\"Categorical: {cat_cols}\")\n",
        "print(f\"Numerical: {num_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRsvxdA6kKr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Missing Values in Numerical Columns:\")\n",
        "print(X_train[num_cols].isnull().sum())\n",
        "\n",
        "print(\"Missing Values in Categorical Columns:\")\n",
        "print(X_train[cat_cols].isnull().sum())\n",
        "\n",
        "# Nullity heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(X_train.isnull(),cbar=False, cmap='viridis')\n",
        "plt.title(\"Nullity Heatmap (Train Set)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COz0UDX3kCOw"
      },
      "source": [
        "**Visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McSZt_kej3gE"
      },
      "outputs": [],
      "source": [
        "# Histograms\n",
        "X_train.hist(figsize=(15, 10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F55naxgdkTJk"
      },
      "source": [
        "Outlier check using box plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onTkCw19j7A4"
      },
      "outputs": [],
      "source": [
        "# Boxplots (Outliers)\n",
        "for col in num_cols:\n",
        "    plt.figure()\n",
        "    sns.boxplot(x=X_train[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgoCNNixloUw"
      },
      "source": [
        "I found outliers in these 3 columns bank_asset_value, residential_assets_value, and commercial_assets_value which are real outliers.\n",
        "\n",
        "\n",
        "people with high income will have more valuable assets. they are rich maybe millionaire or billionaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uXtWJcelNVi"
      },
      "outputs": [],
      "source": [
        "#Correlation Heatmap codedat\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = X_train[num_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFdHmIwfzYGV"
      },
      "source": [
        "Found strong correlation bet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeAw2-9Z9w2X"
      },
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    #Loan to Income ratio\n",
        "    df['loan_to_income_ratio'] = df['loan_amount'] / df['income_annum']\n",
        "\n",
        "    #Aggregate Assets\n",
        "    df['total_assets'] = (df['residential_assets_value'] +\n",
        "                          df['commercial_assets_value'] +\n",
        "                          df['luxury_assets_value'] +\n",
        "                          df['bank_asset_value'])\n",
        "\n",
        "    # 3. Drop Redundant Columns\n",
        "    df = df.drop(columns=['loan_amount', 'income_annum',\n",
        "                          'residential_assets_value', 'commercial_assets_value',\n",
        "                          'luxury_assets_value', 'bank_asset_value'])\n",
        "    return df\n",
        "\n",
        "# Apply to Train AND Test\n",
        "X_train = create_features(X_train)\n",
        "X_test = create_features(X_test)\n",
        "\n",
        "#Updating columns after feature engineering\n",
        "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# check Correlation on the NEW features\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = X_train[num_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap (Post-Feature Engineering)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49Abl0IAAta"
      },
      "source": [
        "I performed feature engineering to address the multicollinearity I found in the heatmap.\n",
        "\n",
        "\n",
        "1. Interaction Feature: I created a loan_to_income ratio. This captures the applicant's repayment burden, which is a much stronger predictor of risk than raw income alone.\n",
        "\n",
        "\n",
        "2. Dimensionality Reduction: Since the asset classes were highly correlated, I aggregated them into total_assets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itl1qhqfBHmn"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szj6WPcFM18X"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss', scale_pos_weight=10)\n",
        "    # Note: XGBoost uses 'scale_pos_weight' instead of 'class_weight' for imbalance\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1GtAVYQM8pe"
      },
      "outputs": [],
      "source": [
        "for name, model in models.items():\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    print(f\"--- {name} Report ---\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Hz3c9UPzcH"
      },
      "outputs": [],
      "source": [
        "# overfitting Check\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Train score\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    train_score = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "    # Test score\n",
        "    y_test_pred = pipeline.predict(X_test)\n",
        "    test_score = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(f\"Train Accuracy: {train_score:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_score:.4f}\")\n",
        "    print(f\"Difference: {abs(train_score - test_score):.4f}\")\n",
        "    print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGQAJIbze-cp"
      },
      "outputs": [],
      "source": [
        "#XGBoost pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', models['XGBoost'])])\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "import shap\n",
        "\n",
        "# 1. Setup Explainer\n",
        "explainer = shap.TreeExplainer(model.named_steps['classifier'])\n",
        "\n",
        "# 2. Transform Data\n",
        "X_train_transformed = model.named_steps['preprocessor'].transform(X_train)\n",
        "if hasattr(X_train_transformed, 'toarray'):\n",
        "    X_train_transformed = X_train_transformed.toarray()\n",
        "\n",
        "# 3. Calculate SHAP values\n",
        "shap_values = explainer.shap_values(X_train_transformed)\n",
        "\n",
        "# 4. Summary Plot\n",
        "shap.summary_plot(shap_values, X_train_transformed, feature_names=model.named_steps['preprocessor'].get_feature_names_out())\n",
        "\n",
        "# 5. Force Plot\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[0,:], X_train_transformed[0,:], feature_names=model.named_steps['preprocessor'].get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1fjT6Esf3wT"
      },
      "outputs": [],
      "source": [
        "# model saving\n",
        "import joblib\n",
        "\n",
        "model_path = '/content/drive/MyDrive/loan predictor/loan_model.joblib'\n",
        "joblib.dump(model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTWeegIZgc92"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "HF_TOKEN = \"TOKEN\"\n",
        "login(HF_TOKEN)\n",
        "\n",
        "repo_name = \"Ravichandrachilde/loan-prediction-XGB\"\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/drive/MyDrive/loan predictor/loan_model.joblib\",\n",
        "    path_in_repo=\"loan_model.joblib\",\n",
        "    repo_id=repo_name,\n",
        "    token=HF_TOKEN,\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
